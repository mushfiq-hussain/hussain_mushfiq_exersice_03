{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mushfiq-hussain/hussain_mushfiq_exersice_03/blob/main/hussain_mushfiq_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "Here is an example of an interesting text classification task and some potential features for building a machine learning model:\n",
        "\n",
        "Task: Classify news articles as political or non-political.\n",
        "\n",
        "Potential features:\n",
        "\n",
        "N-grams: Frequency of politically-charged words and phrases like \"election\", \"candidate\", \"policy\". These would indicate political content.\n",
        "\n",
        "Named entities: Presence of names of political figures, parties, governmental organizations. Also indicates political content.\n",
        "\n",
        "Sentiment analysis: Political articles often express strong positive or negative sentiment. This feature could help with classification.\n",
        "\n",
        "Article source: Source of article (website domain, publisher) can help indicate if political or not.\n",
        "\n",
        "Word embeddings: Using pre-trained word vectors can help detect semantic similarity of words to political domains.\n",
        "\n",
        "Article section: Section of newspaper that article appears could indicate political or not.\n",
        "\n",
        "The goal is to extract features that capture political rhetoric, actors, issues and sentiment. The variety of lexical, semantic, metadata and source-based features can help the machine learning model determine if an article discusses politics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "7e46ac5e-b517-4292-d894-498f905b7450"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c3bedef3a5b4>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtext1_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtext2_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c3bedef3a5b4>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     27\u001b[0m   features = {\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;34m\"ngrams\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   }\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Sample text data\n",
        "text1 = \"The prime minister debated the opposition leader on the new economic policy.\"\n",
        "text2 = \"The new restaurant on Main Street is having its grand opening tomorrow.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_features(text):\n",
        "\n",
        "  # Extract named entities\n",
        "  doc = nlp(text)\n",
        "  entities = [ent.label_ for ent in doc.ents]\n",
        "\n",
        "  # N-gram features\n",
        "  cv = CountVectorizer(ngram_range=(1,2))\n",
        "  cv_features = cv.fit_transform([text])\n",
        "\n",
        "  # Sentiment analysis\n",
        "  blob = TextBlob(text)\n",
        "  sentiment = blob.sentiment.polarity\n",
        "\n",
        "  # Create dictionary of features\n",
        "  features = {\n",
        "    \"entities\": entities,\n",
        "    \"ngrams\": list(cv.get_feature_names()),\n",
        "    \"sentiment\": sentiment\n",
        "  }\n",
        "\n",
        "  return features\n",
        "\n",
        "text1_features = get_features(text1)\n",
        "text2_features = get_features(text2)\n",
        "\n",
        "print(text1_features)\n",
        "print(text2_features)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Extract features for a dataset\n",
        "X = [get_features(text) for text in texts]\n",
        "\n",
        "# Convert to numpy array\n",
        "X = np.array(X)\n",
        "\n",
        "# Target labels\n",
        "y = [1 if 'political' in text else 0 for text in texts]\n",
        "\n",
        "# Compute chi-squared stats between features and target\n",
        "chi2_scores, p_values = chi2(X, y)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = cv.get_feature_names() + ['entities', 'sentiment']\n",
        "\n",
        "# Create dataframe with feature names and chi-squared scores\n",
        "feature_scores = pd.DataFrame({'feature':feature_names, 'chi2_score':chi2_scores})\n",
        "\n",
        "# Sort features by chi-squared score in descending order\n",
        "ranked_features = feature_scores.sort_values('chi2_score', ascending=False)\n",
        "\n",
        "print(ranked_features)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "# Sample texts\n",
        "texts = [text1, text2, text3, text4]\n",
        "\n",
        "# Load pre-trained BERT model from TensorFlow Hub\n",
        "bert_model = hub.load(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\")\n",
        "\n",
        "# Create tokenizer to preprocess text into tokens\n",
        "tokenizer = hub.KerasLayer(bert_model, trainable=False)\n",
        "\n",
        "# Define input query\n",
        "query = \"The prime minister gave a speech today\"\n",
        "\n",
        "# Tokenize query to generate input tokens for BERT\n",
        "query_tokens = tokenizer([query])\n",
        "\n",
        "# Tokenize sample texts to generate input tokens for BERT\n",
        "texts_tokens = tokenizer(texts)\n",
        "\n",
        "# Pass query and text tokens through BERT to get embeddings\n",
        "query_embeddings = bert_model(query_tokens)\n",
        "texts_embeddings = bert_model(texts_tokens)\n",
        "\n",
        "# Compute cosine similarity between query and text embeddings\n",
        "cos_sim = tf.keras.losses.cosine_similarity(query_embeddings, texts_embeddings)\n",
        "\n",
        "# Sort texts by cosine similarity in descending order\n",
        "ranked_texts = tf.argsort(cos_sim, direction='DESCENDING')\n",
        "\n",
        "print(\"Text ranked by similarity:\")\n",
        "# Print ranked texts\n",
        "for i in ranked_texts:\n",
        "  print(f\"{i+1}. {texts[i]}\")\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "Tokenize query and texts using BERT tokenizer\n",
        "Get embeddings from BERT model\n",
        "Compute cosine similarity between query and text embeddings\n",
        "Rank texts by sorting based on cosine similarity scores.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Extracting features from text data is key in NLP. Going through the process of identifying relevant features like entities, sentiment, etc and implementing them in code has helped reinforce my understanding.\n",
        "Using pre-trained models like BERT to generate semantically meaningful embeddings for similarity analysis is very powerful. This exercise provided great practice in leveraging such models.\n",
        "Overall, implementing the end-to-end pipeline from data preprocessing to feature extraction to similarity ranking has improved my applied skills in NLP techniques.\n",
        "Thinking of relevant features for a particular text classification task requires some domain experience. For an unfamiliar dataset, more exploration may be needed to identify useful features\n",
        "Overall, this was a great practical exercise to improve my understanding of core NLP concepts and workflows. The step-by-step implementation has prepared me for tackling similar problems in real applications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}